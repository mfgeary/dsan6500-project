{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "labeling:\n",
    "1 --> Left Neural Foraminal Narrowing\n",
    "2 --> Right Neural Foraminal Narrowing\n",
    "3 --> Left Subarticular Stenosis\n",
    "4 --> Right Subarticular Stenosis\n",
    "5 --> Spinal Canal Stenosis\n",
    "\n",
    "Model Creation and training will be done in this notebook.\n",
    "\"\"\"\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After renaming the preprocessed images to include the condition digit labeling at the end of the filename:\n",
    "\n",
    "1) read in each image\n",
    "2) separate and define label as separate variable\n",
    "3) create Pytorch acceptable image dataset for CNN input later\n",
    "4) split dataset into train, validation, and test (80, 5, 15)\n",
    "\"\"\"\n",
    "\n",
    "# class steps were followed from here: \n",
    "# https://stackoverflow.com/questions/67406731/pytorch-import-dataset-with-images-as-labels\n",
    "\n",
    "class Create_Dataset(Dataset): \n",
    "\n",
    "    def __init__(self, root_path, transform=None):\n",
    "        self.root_path = root_path\n",
    "        self.data_paths = [f for f in sorted(os.listdir(root_path)) if f.endswith(\".png\")]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #extract label\n",
    "        img_file = self.data_paths[idx]\n",
    "        label = int(img_file.replace('.png', '').split('_')[-1])\n",
    "        #read in the img\n",
    "        img_name = self.data_paths[idx]\n",
    "        img_path = os.path.join(self.root_path, img_name)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "#creat the class object\n",
    "DATA_PATH = '../../data/preprocessed_renamed'\n",
    "data = Create_Dataset(root_path = DATA_PATH, \n",
    "                      transform = transforms.Compose([transforms.Resize((100, 100)), # for some reason some image sizes are not 100 x 100\n",
    "                                                      transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the mean and std of entire dataset for normalization.\n",
    "\n",
    "Shawn: This ran for about 4 min\n",
    "\"\"\"\n",
    "\n",
    "#dataload the entire dataset in order to calculate mean and std for normalization\n",
    "all_data_loader = DataLoader(data, batch_size=500, shuffle=False)\n",
    "\n",
    "\n",
    "# steps followed from here:\n",
    "# https://saturncloud.io/blog/how-to-normalize-image-dataset-using-pytorch/\n",
    "\n",
    "def get_mean_std(loader):\n",
    "    # Compute the mean and standard deviation of all pixels in the dataset\n",
    "    num_pixels = 0\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    for images, _ in loader:\n",
    "        batch_size, _, height, width = images.shape\n",
    "        num_pixels += batch_size * height * width\n",
    "        mean += images.mean(axis=(0, 2, 3)).sum()\n",
    "        std += images.std(axis=(0, 2, 3)).sum()\n",
    "\n",
    "    mean /= num_pixels\n",
    "    std /= num_pixels\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "mean, std = get_mean_std(all_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Normalize the entire dataset when initializing dataset object\n",
    "\"\"\"\n",
    "\n",
    "data = Create_Dataset(root_path = DATA_PATH, \n",
    "                      transform = transforms.Compose([transforms.Resize((100, 100)), # for some reason some image sizes are not 100 x 100\n",
    "                                                      transforms.ToTensor(),\n",
    "                                                      transforms.Normalize(mean=mean, std=std)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After normalizing, split the dataset to train, val, and testing (80, 5, 15)\n",
    "\"\"\"\n",
    "\n",
    "#setup random split sizing\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = int(0.05 * len(data))\n",
    "test_size = len(data) - train_size - val_size\n",
    "\n",
    "#random split dataset with random_split()\n",
    "train, val, test = random_split(data, [train_size, val_size, test_size])\n",
    "\n",
    "#create dataloader for training, validation, and testing\n",
    "train_loader = DataLoader(train, batch_size=500, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=500, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=500, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
